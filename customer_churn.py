# -*- coding: utf-8 -*-
"""customer_churn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16sA-fcraxGxAoVrlacF2JB9ukpVY81Es

# ***Theme of This Project***
This project predicts whether a telecom/e-commerce customer will churn (leave) using historical customer usage and account data. The pipeline begins with raw CSV data ingestion, cleaning and feature engineering (handling missing values, encoding categorical variables, scaling numeric features), followed by exploratory data analysis (to understand feature distributions and class imbalance). Several supervised models (Random Forest, Gradient Boosting, and an artificial neural network) are trained and tuned. Models are evaluated with metrics appropriate for imbalanced classification (precision, recall, F1, ROC-AUC, PR-AUC). The best performing model is saved and documented for deployment; visualization and model interpretation (feature importances, ROC, confusion matrix) support business decisions to reduce churn.

# ***Step-by-step process***

Data acquisition

Load the CSV (e.g., WA_Fn-UseC_-Telco-Customer-Churn.csv) from Drive or Colab upload.

Initial inspection

df.head(), df.info(), df.describe(), df.isnull().sum() to find missing values and types.

Cleaning & missing value handling

Impute numeric missing values (median/mean) and categorical (mode or new category “Unknown”).

Fix inconsistent strings (e.g., trim spaces, unify case).

Feature engineering

Create meaningful features (e.g., avg_day_charge = total_day_charge / total_day_minutes), bin continuous features if useful, combine or drop redundant columns (IDs).

Categorical encoding

For ordinal categories use OrdinalEncoder; for nominal use OneHotEncoder or target encoding if many categories.

Scaling

Apply StandardScaler or MinMaxScaler to numeric features (especially for ANN).

Train/test split & resampling

train_test_split (e.g., 80/20). Handle class imbalance with class_weight or oversampling (SMOTE) on the training set only.

Model training

Train Random Forest, Gradient Boosting (XGBoost/LightGBM/CatBoost), and a simple ANN. Use cross-validation and hyperparameter search (GridSearchCV/RandomizedSearchCV).

Evaluation

Use accuracy, precision, recall, F1, ROC-AUC, PR-AUC. Plot confusion matrix & ROC. Prefer recall/precision depending on business cost of false negatives/positives.

Importing all the neccesary packages
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot
import cufflinks as cf # Works as a connector between the pandas library and plotly
cf.go_offline()
from sklearn.model_selection import GridSearchCV

"""Reading DataSet"""

df = pd.read_csv('/content/telecom_churn.csv')
df.head(5)

"""DataSet Info"""

df.info()
df.describe()

"""Visual represtation of dataset(Histogram)"""

df.hist(figsize=(15,15))
plt.show()

"""Visual reapresentaion of Dataset(Pie chart)

---


"""

colors = sns.color_palette('pastel')[0:5]
plt.pie(df['class'].value_counts(), labels = ['Retained(0)', 'Exited(1)'], colors = colors, autopct='%.0f%%')
plt.show()

"""Visual reapresentaion of Dataset(HeatMap)

---


"""

corr_matrix = df.corr()
plt.figure(figsize = (15,15))
sns.heatmap(corr_matrix, annot = True, fmt = '0.2f')
plt.show()

"""day_charges_churn_distribution(Kernel Density Estimate (KDE) plot)"""

# Churn by day charges
ax = sns.kdeplot(df.total_day_charge[(df["class"] == 0)],
               color = "Red", shade = True)
ax = sns.kdeplot(df.total_day_charge[(df["class"] == 1)],
               color = "Blue", shade = True)

ax.legend(["Retain", "Exited"], loc = "upper right")
ax.set_ylabel("Density")
ax.set_xlabel("Day Charges")
ax.set_title("Distribution of day charges by churn")

"""“Comparative KDE Distribution Plot” for evening charges."""

# Churn by evening charges
ax = sns.kdeplot(df.total_eve_charge[(df["class"] == 0)],
               color = "Red", shade = True)
ax = sns.kdeplot(df.total_eve_charge[(df["class"] == 1)],
               color = "Blue", shade = True)

ax.legend(["Retain", "Exited"], loc = "upper right")
ax.set_ylabel("Density")
ax.set_xlabel("Evening Charges")
ax.set_title("Distribution of evening charges by churn")

"""**Telecom Churn Prediction : Data Preprocessing**"""

X = df.drop(['class', 'area_code', 'phone_number'], axis='columns')
Y = df['class']

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2)

from sklearn.ensemble import RandomForestClassifier
RF = RandomForestClassifier()
RF.fit(x_train,y_train)

"""**Telecom Churn Prediction : Feature Selection**"""

from sklearn.ensemble import RandomForestClassifier
RF = RandomForestClassifier()
RF.fit(x_train,y_train)

# Plot the feature importance
feature_scores= pd.DataFrame({"Fraction of variables affected" : RF.feature_importances_},index = X.columns)
feature_scores= feature_scores.sort_values(by = "Fraction of variables affected")
feature_scores.plot(kind = "barh", figsize = (10, 5))
sns.despine()

"""**Telecom Churn Prediction : Model Evaluation**

**- Logistic Regression**
"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix

Grid={"C":np.logspace(-3,3,7), "penalty":["l1","l2"]}# l1 lasso l2 ridge
LR_Model = LogisticRegression()
LR_Model_CV=GridSearchCV(LR_Model,Grid,cv=10)

LR_Model_CV.fit(x_train, y_train)

print("tuned hpyerparameters :(best parameters) ",LR_Model_CV.best_params_)

LR_Model = LogisticRegression(C=0.001, penalty="l2")
LR_Model.fit(x_train, y_train)


y_pred = LR_Model.predict(x_test)
print(classification_report(y_test, y_pred))

cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot = True)

"""**Telecom Churn Prediction : Model Evaluation**

**- Support Vector Machine**
"""

from sklearn.calibration import CalibratedClassifierCV
from sklearn.svm import LinearSVC

SVM_Model = LinearSVC(max_iter = 10000)
SVM_Model = CalibratedClassifierCV()
SVM_Model.fit(x_train, y_train)

y_pred = SVM_Model.predict(x_test)
print(classification_report(y_test, y_pred))

cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot = True)

"""**Telecom Churn Prediction : Model Evaluation**

**- Random Forest Classifier**
"""

from sklearn.ensemble import RandomForestClassifier

RF_Model = RandomForestClassifier()
RF_Model.fit(x_train, y_train)

y_pred = RF_Model.predict(x_test)
print(classification_report(y_test, y_pred))

cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot = True)

"""**Telecom Churn Prediction : Model Evaluation**

**- K-Nearest Neighbour**
"""

from sklearn.neighbors import KNeighborsClassifier

KNN = KNeighborsClassifier()
k_range = list(range(1, 31))
param_grid = dict(n_neighbors=k_range)

# defining parameter range
grid = GridSearchCV(KNN, param_grid, cv=10, scoring='accuracy', return_train_score=False,verbose=1)

# fitting the model for grid search
grid_search=grid.fit(x_train, y_train)

print(grid_search.best_params_)

KNN_Model = KNeighborsClassifier(n_neighbors=6)
KNN_Model.fit(x_train, y_train)

y_pred = KNN_Model.predict(x_test)
print(classification_report(y_test, y_pred))

cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot = True)

"""**Telecom Churn Prediction : Model Evaluation**

**- Naive Bayes Classifier**
"""

from sklearn.naive_bayes import GaussianNB

GNB_Model = GaussianNB()
GNB_Model.fit(x_train, y_train)

y_pred = GNB_Model.predict(x_test)
print(classification_report(y_test, y_pred))

cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot = True)

"""**Comparing Models**"""

# ROC curve
from sklearn.metrics import roc_curve

fpr1, tpr1, thresh1 = roc_curve(y_test, LR_Model.predict_proba(x_test)[:, 1], pos_label = 1)
fpr2, tpr2, thresh2 = roc_curve(y_test, SVM_Model.predict_proba(x_test)[:, 1], pos_label = 1)
fpr3, tpr3, thresh3 = roc_curve(y_test, RF_Model.predict_proba(x_test)[:, 1], pos_label = 1)
fpr4, tpr4, thresh4 = roc_curve(y_test, KNN_Model.predict_proba(x_test)[:, 1], pos_label = 1)
fpr5, tpr5, thresh5 = roc_curve(y_test, GNB_Model.predict_proba(x_test)[:, 1], pos_label = 1)

# AUC score
from sklearn.metrics import roc_auc_score

auc_score1 = roc_auc_score(y_test, LR_Model.predict_proba(x_test)[:, 1])
auc_score2 = roc_auc_score(y_test, SVM_Model.predict_proba(x_test)[:, 1])
auc_score3 = roc_auc_score(y_test, RF_Model.predict_proba(x_test)[:, 1])
auc_score4 = roc_auc_score(y_test, KNN_Model.predict_proba(x_test)[:, 1])
auc_score5 = roc_auc_score(y_test, GNB_Model.predict_proba(x_test)[:, 1])

print("Logistic Regression: ", auc_score1) # Logistic Regression
print("Support Vector Machine: ", auc_score2) # Support Vector Machine
print("Random Forest: ", auc_score3) # Random Forest
print("K-Nearest Neighbors: ", auc_score4) # K-Nearest Neighbors
print("Naive Bayes: ", auc_score5) # Naive Bayes

plt.plot(fpr1, tpr1, linestyle = "--", color = "orange", label = "Logistic Regression")
plt.plot(fpr2, tpr2, linestyle = "--", color = "red", label = "SVM")
plt.plot(fpr3, tpr3, linestyle = "--", color = "green", label = "Random Forest")
plt.plot(fpr4, tpr4, linestyle = "--", color = "yellow", label = "KNN")
plt.plot(fpr5, tpr5, linestyle = "--", color = "black", label = "Naive bayes")

plt.title('Receiver Operator Characteristics (ROC)')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive rate')

plt.legend(loc = 'best')
plt.savefig('ROC', dpi = 300)
plt.show()

"""# ***Models used***
***Random Forest***

What it is: Ensemble of decision trees using bagging; robust to noise and needs little scaling.

Why use it: Good baseline, handles mixed data types, interpretable with feature importances.

Tune: n_estimators, max_depth, min_samples_split, max_features.

Pros/Cons: Fast to train for moderate sized data; can overfit if not tuned. Works well with imbalanced data if class_weight='balanced'.

***Gradient Boosting (XGBoost / LightGBM / CatBoost)***

What it is: Boosted trees that sequentially fix prior errors — often top performer on tabular data.

Why use it: High predictive power, handles missing values (CatBoost/LightGBM) and categorical data (CatBoost).

Tune: learning_rate, n_estimators, max_depth, subsample, colsample_bytree.

Pros/Cons: Usually better accuracy than RF but needs more careful tuning and longer training time.

Artificial Neural Network (ANN) **bold text** *italicized text*

What it is: Feedforward MLP or shallow NN built with TensorFlow/Keras or PyTorch.

Why use it: Flexible; useful if you create many numeric features or combine with embeddings for high-cardinality categorical variables.

Architecture tips: Start simple — e.g., Dense(64) → Dense(32) → Dense(1, sigmoid). Use dropout, batchnorm, early stopping.

Tune: number of layers, neurons, dropout rate, batch size, learning rate.

Pros/Cons: May need more data & tuning; less interpretable than tree models.

## Plots used
KDE plot — distribution by churn (day/evening/night charges)
***bold text***
What it is: Kernel Density Estimate (smoothed distribution).

Why use it: Compare distributions of continuous features across churn vs. retain groups.

Interpretation: Overlap means similar behavior; separation suggests predictive power.


Histogram (single variable distributions) ***bold text***

What it is: Counts per bin for a numeric variable.

Why use it: See skewness, multi-modality, or outliers.

When to use: Before/after log transforms or binning.

***Boxplot (numeric vs churn)***

What it is: Shows median, IQR, and outliers.

Why use it: Compare median and spread across churn groups (detect outliers).

***Correlation heatmap***

What it is: Pearson (or Spearman) correlations between numeric features.

Why use it: Identify highly correlated or redundant features for removal or PCA.

Countplot / Bar chart (categorical distributions)

What it is: Frequency of categories split by churn.

Why use it: Identify categorical levels associated with churn (e.g., contract type).

***Feature importance plot (tree models)***

What it is: Importance score per feature from RF/XGBoost.

Why use it: Understand which features contribute most to prediction; help business take action.

Confusion matrix & classification report

What it is: True vs predicted class matrix + precision/recall/f1.

Why use it: Understand types of errors; important for business risk analysis.

***ROC Curve & AUC***

What it is: True positive rate vs false positive rate across thresholds.

Why use it: Overall discrimination ability of model; AUC is threshold-independent metric.

***Precision-Recall Curve & PR-AUC***

What it is: Precision vs recall at different thresholds.

Why use it: Preferable when data is imbalanced (churn typically is).

# ***Project value (Learning curve)***

What it is: Train and validation score as function of training set size.

Why use it: Diagnose high bias vs high variance and whether more data helps.

SHAP / Partial Dependence Plots (interpretability)

What it is: Explain model predictions (global & local).

Why use it: Show how a feature influences predicted churn probability — useful for stakeholders.
"""